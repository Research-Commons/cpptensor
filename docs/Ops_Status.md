# cpptensor Operations Catalog

**Comprehensive inventory of implemented and missing tensor operations**

**Last Updated:** November 5, 2025  
**Repository:** cpptensor  
**Status:** Active Development

---

## Table of Contents

1. [âœ… Implemented Operations](#implemented-operations)
2. [âŒ Missing Operations](#missing-operations)
3. [ðŸ”„ Partially Implemented](#partially-implemented)
4. [ðŸ“‹ Operation Categories](#operation-categories)
5. [ðŸŽ¯ Priority Roadmap](#priority-roadmap)

---

## âœ… Implemented Operations

### 1. Tensor Creation & Initialization

| Operation | Function | Description | Status |
|-----------|----------|-------------|--------|
| **From data** | `Tensor(shape, values)` | Create from vector of values | âœ… Working |
| **Zeros** | `Tensor::zeros(shape)` | Create tensor filled with zeros | âœ… Working |
| **Ones** | `Tensor::ones(shape)` | Create tensor filled with ones | âœ… Working |
| **Full** | `Tensor::full(shape, value)` | Create tensor filled with scalar | âœ… Working |
| **Random normal** | `Tensor::randn(shape)` | Random values from N(0,1) | âœ… Working |
| **Random uniform** | `Tensor::rand(shape)` | Random values from U(0,1) | âœ… Working |
| **From pointer** | `Tensor::from_ptr(shape, ptr, owner)` | Zero-copy view from raw pointer | âœ… Working |

### 2. Element-wise Arithmetic Operations

| Operation | Operator/Function | Description | Status |
|-----------|------------------|-------------|--------|
| **Addition** | `A + B`, `add(A, B)` | Element-wise addition | âœ… Working |
| **Subtraction** | `A - B`, `sub(A, B)` | Element-wise subtraction | âœ… Working |
| **Multiplication** | `A * B`, `mul(A, B)` | Element-wise multiplication (Hadamard) | âœ… Working |
| **Division** | `A / B`, `div(A, B)` | Element-wise division | âœ… Working |
| **Power** | `pow(A, B)` | Element-wise power | âœ… Working |
| **Negation** | `-A`, `neg(A)` | Element-wise negation | âœ… Working |

### 3. Mathematical Functions (Unary)

| Operation | Function | Description | Status |
|-----------|----------|-------------|--------|
| **Exponential** | `exp(A)` | e^x for each element | âœ… Working |
| **Natural log** | `log(A)` | ln(x) for each element | âœ… Working |
| **Square root** | `sqrt(A)` | âˆšx for each element | âœ… Working |
| **Absolute value** | `abs(A)` | \|x\| for each element | âœ… Working |
| **Sine** | `sin(A)` | sin(x) for each element | âœ… Working |
| **Cosine** | `cos(A)` | cos(x) for each element | âœ… Working |
| **Tangent** | `tan(A)` | tan(x) for each element | âœ… Working |

### 4. Activation Functions

| Operation | Function | Description | Status |
|-----------|----------|-------------|--------|
| **ReLU** | `relu(A)` | max(0, x) for each element | âœ… Working |
| **Sigmoid** | `sigmoid(A)` | 1 / (1 + e^(-x)) | âœ… Working |

### 5. Linear Algebra Operations

| Operation | Function | Description | Status |
|-----------|----------|-------------|--------|
| **Matrix multiply** | `matmul(A, B)` | Matrix multiplication (2D and ND batched) | âœ… Working (Optimized) |
| **Dot product** | `dot(A, B)` | Vector dot product (1D tensors) | âœ… Working |
| **Tensor dot** | `tensordot(A, B, axes)` | Generalized tensor contraction | âœ… Working |
| **SVD** | `svd(A, full_matrices, compute_uv)` | Singular value decomposition | âœ… Working |
| **Eigenvalue** | `eig(A, compute_eigenvectors)` | General eigenvalue decomposition | âœ… Working |
| **Symmetric eig** | `eig_symmetric(A, compute_eigenvectors)` | Symmetric eigenvalue decomposition | âœ… Working |

### 6. Tensor Manipulation (Views & Reshaping)

| Operation | Method | Description | Status |
|-----------|--------|-------------|--------|
| **View** | `A.view(new_shape)` | Reshape without copying (zero-copy) | âœ… Working |
| **Reshape** | `A.reshape(new_shape)` | Change shape (may copy if not contiguous) | âœ… Working |
| **Flatten** | `A.flatten(start_dim, end_dim)` | Flatten dimensions into 1D | âœ… Working |
| **Squeeze** | `A.squeeze(dim)` | Remove size-1 dimensions | âœ… Working |
| **Unsqueeze** | `A.unsqueeze(dim)` | Add size-1 dimension | âœ… Working |
| **Permute** | `A.permute(dims)` | Reorder dimensions (generalized transpose) | âœ… Working |
| **Transpose** | `A.transpose(dim0, dim1)` | Swap two dimensions | âœ… Working |
| **Contiguous** | `A.contiguous()` | Ensure contiguous memory layout | âœ… Working |
| **Clone** | `A.clone()` | Deep copy of tensor | âœ… Working |

### 7. Tensor Properties & Inspection

| Operation | Method | Description | Status |
|-----------|--------|-------------|--------|
| **Shape** | `A.shape()` | Get tensor dimensions | âœ… Working |
| **Size/numel** | `A.numel()` | Total number of elements | âœ… Working |
| **Dimensions** | `A.ndim()` | Number of dimensions | âœ… Working |
| **Device** | `A.device_type()` | Get device (CPU/CUDA) | âœ… Working |
| **Strides** | `A.strides()` | Get stride information | âœ… Working |
| **Is contiguous** | `A.is_contiguous()` | Check if contiguous layout | âœ… Working |
| **Data access** | `A.data()` | Access underlying data vector | âœ… Working |
| **Print** | `A.print()` | Print tensor contents | âœ… Working |

### 8. Backend & Dispatch

| Operation | Function | Description | Status |
|-----------|----------|-------------|--------|
| **Initialize kernels** | `initialize_kernels()` | Load backend (OpenBLAS/AVX/CUDA) | âœ… Working |
| **Backend selection** | Device type enum | CPU, CUDA, AVX2, AVX512 | âœ… Working |

---

## âŒ Missing Operations

### 1. Reduction Operations (High Priority)

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Sum** | `A.sum(dim, keepdim)` | Sum along dimension(s) | ðŸ”´ P0 |
| **Mean** | `A.mean(dim, keepdim)` | Average along dimension(s) | ðŸ”´ P0 |
| **Max** | `A.max(dim, keepdim)` | Maximum along dimension | ðŸŸ¡ P1 |
| **Min** | `A.min(dim, keepdim)` | Minimum along dimension | ðŸŸ¡ P1 |
| **Argmax** | `A.argmax(dim, keepdim)` | Index of maximum value | ðŸŸ¡ P1 |
| **Argmin** | `A.argmin(dim, keepdim)` | Index of minimum value | ðŸŸ¡ P1 |
| **Prod** | `A.prod(dim, keepdim)` | Product along dimension | ðŸŸ¢ P2 |
| **Std** | `A.std(dim, keepdim)` | Standard deviation | ðŸŸ¢ P2 |
| **Var** | `A.var(dim, keepdim)` | Variance | ðŸŸ¢ P2 |
| **Norm** | `A.norm(p, dim)` | p-norm along dimension | ðŸŸ¢ P2 |

**Impact:** Critical for neural networks (softmax, normalization layers)

### 2. Tensor Manipulation (High Priority)

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Concatenate** | `torch.cat([A, B], dim)` | Join tensors along dimension | ðŸ”´ P0 |
| **Stack** | `torch.stack([A, B], dim)` | Stack tensors (new dimension) | ðŸ”´ P0 |
| **Split** | `torch.split(A, size, dim)` | Split tensor into chunks | ðŸŸ¡ P1 |
| **Chunk** | `torch.chunk(A, chunks, dim)` | Split into N chunks | ðŸŸ¡ P1 |
| **Expand** | `A.expand(shape)` | Broadcast to new shape (no copy) | ðŸŸ¡ P1 |
| **Repeat** | `A.repeat(counts)` | Repeat tensor along dimensions | ðŸŸ¡ P1 |
| **Tile** | `A.tile(reps)` | Repeat entire tensor | ðŸŸ¢ P2 |
| **Gather** | `torch.gather(A, dim, idx)` | Gather values along dimension | ðŸŸ¢ P2 |
| **Scatter** | `torch.scatter(A, dim, idx, val)` | Scatter values along dimension | ðŸŸ¢ P2 |

**Impact:** Essential for advanced network architectures (ResNet, Transformers)

### 3. Indexing & Slicing (High Priority)

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Basic slicing** | `A[0:10, :, 5]` | NumPy-style slicing | ðŸ”´ P0 |
| **Advanced indexing** | `A[[0, 2, 4], [1, 3, 5]]` | Index with arrays | ðŸŸ¡ P1 |
| **Boolean indexing** | `A[mask]` | Index with boolean mask | ðŸŸ¡ P1 |
| **Where** | `torch.where(cond, A, B)` | Select elements based on condition | ðŸŸ¡ P1 |
| **Masked select** | `torch.masked_select(A, mask)` | Select elements where mask is true | ðŸŸ¢ P2 |
| **Index select** | `torch.index_select(A, dim, idx)` | Select along dimension | ðŸŸ¢ P2 |

**Impact:** Critical for data manipulation and masking in NLP/Vision

### 4. Comparison & Logical Operations

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Equal** | `A == B`, `torch.eq(A, B)` | Element-wise equality | ðŸŸ¡ P1 |
| **Not equal** | `A != B`, `torch.ne(A, B)` | Element-wise inequality | ðŸŸ¡ P1 |
| **Greater** | `A > B`, `torch.gt(A, B)` | Element-wise greater than | ðŸŸ¡ P1 |
| **Less** | `A < B`, `torch.lt(A, B)` | Element-wise less than | ðŸŸ¡ P1 |
| **Greater/equal** | `A >= B`, `torch.ge(A, B)` | Element-wise >= | ðŸŸ¡ P1 |
| **Less/equal** | `A <= B`, `torch.le(A, B)` | Element-wise <= | ðŸŸ¡ P1 |
| **Logical AND** | `torch.logical_and(A, B)` | Element-wise AND | ðŸŸ¢ P2 |
| **Logical OR** | `torch.logical_or(A, B)` | Element-wise OR | ðŸŸ¢ P2 |
| **Logical NOT** | `torch.logical_not(A)` | Element-wise NOT | ðŸŸ¢ P2 |
| **Allclose** | `torch.allclose(A, B)` | Check approximate equality | ðŸŸ¢ P2 |

**Impact:** Needed for control flow and validation

### 5. Advanced Activation Functions

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Tanh** | `torch.tanh(A)` | Hyperbolic tangent | ðŸŸ¡ P1 |
| **Softmax** | `torch.softmax(A, dim)` | Softmax along dimension | ðŸ”´ P0 |
| **Log softmax** | `torch.log_softmax(A, dim)` | Log of softmax (numerically stable) | ðŸŸ¡ P1 |
| **Leaky ReLU** | `torch.nn.functional.leaky_relu(A)` | Leaky ReLU | ðŸŸ¡ P1 |
| **ELU** | `torch.nn.functional.elu(A)` | Exponential Linear Unit | ðŸŸ¢ P2 |
| **GELU** | `torch.nn.functional.gelu(A)` | Gaussian Error Linear Unit | ðŸŸ¡ P1 |
| **Swish/SiLU** | `torch.nn.functional.silu(A)` | Sigmoid Linear Unit | ðŸŸ¢ P2 |
| **Softplus** | `torch.nn.functional.softplus(A)` | Smooth approximation of ReLU | ðŸŸ¢ P2 |

**Impact:** Essential for modern deep learning (GELU in Transformers)

### 6. Broadcasting & Type Conversions

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Broadcasting** | Automatic | NumPy-style broadcasting | ðŸ”´ P0 |
| **Type casting** | `A.to(dtype)` | Convert data type | ðŸŸ¡ P1 |
| **Device transfer** | `A.to(device)` | Move tensor to device | ðŸŸ¡ P1 |
| **Fill** | `A.fill_(value)` | Fill tensor with scalar | ðŸŸ¢ P2 |
| **Copy** | `A.copy_(B)` | Copy B's data into A | ðŸŸ¢ P2 |

**Impact:** Broadcasting critical for vectorized operations

### 7. Linear Algebra (Advanced)

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **QR decomposition** | `torch.linalg.qr(A)` | QR factorization | ðŸŸ¢ P2 |
| **Cholesky** | `torch.linalg.cholesky(A)` | Cholesky decomposition | ðŸŸ¢ P2 |
| **LU decomposition** | `torch.linalg.lu(A)` | LU factorization | ðŸŸ¢ P2 |
| **Matrix inverse** | `torch.linalg.inv(A)` | Matrix inverse | ðŸŸ¡ P1 |
| **Determinant** | `torch.linalg.det(A)` | Matrix determinant | ðŸŸ¢ P2 |
| **Matrix rank** | `torch.linalg.matrix_rank(A)` | Rank of matrix | ðŸŸ¢ P2 |
| **Solve linear** | `torch.linalg.solve(A, b)` | Solve Ax = b | ðŸŸ¡ P1 |
| **Cross product** | `torch.cross(A, B)` | Cross product | ðŸŸ¢ P3 |
| **Outer product** | `torch.outer(A, B)` | Outer product | ðŸŸ¢ P3 |

**Impact:** Useful for numerical computing, less critical for DL

### 8. Convolution Operations

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Conv1d** | `torch.nn.functional.conv1d` | 1D convolution | ðŸŸ¡ P1 |
| **Conv2d** | `torch.nn.functional.conv2d` | 2D convolution | ðŸ”´ P0 |
| **Conv3d** | `torch.nn.functional.conv3d` | 3D convolution | ðŸŸ¢ P2 |
| **ConvTranspose2d** | `torch.nn.functional.conv_transpose2d` | Transposed convolution | ðŸŸ¡ P1 |
| **MaxPool2d** | `torch.nn.functional.max_pool2d` | 2D max pooling | ðŸ”´ P0 |
| **AvgPool2d** | `torch.nn.functional.avg_pool2d` | 2D average pooling | ðŸŸ¡ P1 |
| **AdaptiveAvgPool** | `torch.nn.functional.adaptive_avg_pool2d` | Adaptive pooling | ðŸŸ¡ P1 |

**Impact:** Critical for CNN architectures

### 9. Loss Functions

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **MSE Loss** | `torch.nn.functional.mse_loss` | Mean squared error | ðŸ”´ P0 |
| **Cross entropy** | `torch.nn.functional.cross_entropy` | Cross entropy loss | ðŸ”´ P0 |
| **BCE Loss** | `torch.nn.functional.binary_cross_entropy` | Binary cross entropy | ðŸŸ¡ P1 |
| **L1 Loss** | `torch.nn.functional.l1_loss` | L1 (MAE) loss | ðŸŸ¡ P1 |
| **KL Divergence** | `torch.nn.functional.kl_div` | KL divergence | ðŸŸ¢ P2 |
| **NLL Loss** | `torch.nn.functional.nll_loss` | Negative log likelihood | ðŸŸ¢ P2 |

**Impact:** Essential for training neural networks

### 10. Normalization Operations

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Batch norm** | `torch.nn.functional.batch_norm` | Batch normalization | ðŸ”´ P0 |
| **Layer norm** | `torch.nn.functional.layer_norm` | Layer normalization | ðŸŸ¡ P1 |
| **Instance norm** | `torch.nn.functional.instance_norm` | Instance normalization | ðŸŸ¢ P2 |
| **Group norm** | `torch.nn.functional.group_norm` | Group normalization | ðŸŸ¢ P2 |

**Impact:** Critical for modern architectures (Transformers, ResNets)

### 11. Autograd & Gradient Operations

| Operation | PyTorch Equivalent | Description | Priority |
|-----------|-------------------|-------------|----------|
| **Backward** | `loss.backward()` | Compute gradients | ðŸ”´ P0 |
| **Grad accumulation** | `A.grad` | Access gradient | ðŸ”´ P0 |
| **Zero grad** | `optimizer.zero_grad()` | Clear gradients | ðŸ”´ P0 |
| **Detach** | `A.detach()` | Detach from computation graph | ðŸŸ¡ P1 |
| **No grad context** | `torch.no_grad()` | Disable gradient tracking | ðŸŸ¡ P1 |
| **Gradient clipping** | `torch.nn.utils.clip_grad_norm_` | Clip gradients | ðŸŸ¡ P1 |

**Impact:** Fundamental for training (currently partially implemented)

---

## ðŸ”„ Partially Implemented

### Autograd System
- **Status:** Infrastructure exists but incomplete
- **What works:** Basic gradient tracking hooks (`grad_fn`)
- **What's missing:**
    - Backward pass implementation
    - Gradient accumulation
    - Higher-order derivatives
- **Files:** `include/cpptensor/autograd/`

### Broadcasting
- **Status:** Not implemented
- **What works:** Operations on same-shaped tensors
- **What's missing:**
    - Automatic shape broadcasting
    - Broadcasting rules (NumPy-compatible)
- **Impact:** Limits vectorization and expressiveness

### Device Management
- **Status:** Enum exists, limited functionality
- **What works:** Device type specification (CPU/CUDA)
- **What's missing:**
    - Actual CUDA implementation
    - Device-to-device transfers
    - Multi-GPU support
- **Files:** `include/cpptensor/enums/dispatcherEnum.h`

---

## ðŸ“‹ Operation Categories Summary

| Category | Implemented | Missing | Coverage |
|----------|-------------|---------|----------|
| **Creation** | 7 | 0 | 100% âœ… |
| **Arithmetic** | 6 | 0 | 100% âœ… |
| **Math (Unary)** | 7 | 0 | 100% âœ… |
| **Activation** | 2 | 7 | 22% âš ï¸ |
| **Linear Algebra** | 6 | 9 | 40% âš ï¸ |
| **Manipulation** | 9 | 9 | 50% âš ï¸ |
| **Reduction** | 0 | 10 | 0% âŒ |
| **Comparison** | 0 | 10 | 0% âŒ |
| **Indexing** | 0 | 6 | 0% âŒ |
| **Convolution** | 0 | 7 | 0% âŒ |
| **Loss Functions** | 0 | 6 | 0% âŒ |
| **Normalization** | 0 | 4 | 0% âŒ |
| **Autograd** | 10% | 90% | 10% âŒ |

**Overall Coverage:** ~35% of essential PyTorch operations

---

## ðŸŽ¯ Priority Roadmap

### Phase 1: Core Operations (Next Sprint) ðŸ”´

**Goal:** Enable basic neural network training

| Priority | Operation | Reason | Estimated Effort |
|----------|-----------|--------|------------------|
| **P0** | `sum(dim)`, `mean(dim)` | Needed for loss functions | 2-3 days |
| **P0** | `softmax(dim)` | Essential for classification | 1-2 days |
| **P0** | Broadcasting support | Critical for vectorization | 3-5 days |
| **P0** | `cat(tensors, dim)` | Data manipulation | 2 days |
| **P0** | Basic slicing `A[i:j]` | Tensor indexing | 3-4 days |
| **P0** | Cross entropy loss | Training loss | 2 days |
| **P0** | MSE loss | Regression loss | 1 day |

**Total:** ~15-20 days

### Phase 2: Neural Network Essentials (Month 2) ðŸŸ¡

**Goal:** Support CNN and basic architectures

| Priority | Operation | Reason | Estimated Effort |
|----------|-----------|--------|------------------|
| **P1** | `conv2d` | Convolutional layers | 5-7 days |
| **P1** | `max_pool2d` | Pooling layers | 2-3 days |
| **P1** | `batch_norm` | Normalization | 3-4 days |
| **P1** | `max/min(dim)` | Pooling & statistics | 2 days |
| **P1** | Comparison ops (`>`, `<`, etc.) | Control flow | 2-3 days |
| **P1** | `tanh`, `leaky_relu` | More activations | 1-2 days |
| **P1** | `stack`, `split` | Data manipulation | 3 days |

**Total:** ~18-25 days

### Phase 3: Advanced Features (Month 3) ðŸŸ¢

**Goal:** Transformer support and optimization

| Priority | Operation | Reason | Estimated Effort |
|----------|-----------|--------|------------------|
| **P2** | `layer_norm` | Transformer architecture | 2-3 days |
| **P2** | `gelu` activation | Modern activations | 1 day |
| **P2** | `einsum` | Flexible tensor ops | 5-7 days |
| **P2** | Advanced indexing | Complex slicing | 4-5 days |
| **P2** | `gather`, `scatter` | Embedding layers | 3-4 days |
| **P2** | Gradient clipping | Training stability | 2 days |

**Total:** ~17-25 days

### Phase 4: Production Readiness (Month 4+)

- Complete autograd backward pass
- CUDA implementation
- Optimization passes (fusion, memory planning)
- Comprehensive testing
- Documentation
- Benchmarking suite

---

## Implementation Notes

### Quick Wins (< 1 day each)
1. `tanh` - just wrap `std::tanh`
2. `neg` - already implemented as unary `-`
3. `MSE loss` - simple: `mean((A - B).pow(2))`
4. `L1 loss` - simple: `mean(abs(A - B))`
5. `fill_(value)` - straightforward loop

### Medium Complexity (2-4 days)
1. `sum(dim)` - reduction with axis handling
2. `softmax(dim)` - exp + normalize (watch numerical stability!)
3. `cat/stack` - memory layout + copying
4. Basic slicing - view system extension
5. `conv2d` - use existing im2col or direct approach

### High Complexity (5+ days)
1. Broadcasting - affects entire operation system
2. Advanced indexing - complex memory patterns
3. `einsum` - generalized tensor contraction
4. Autograd backward - dependency graph traversal
5. CUDA kernels - entirely new backend

---

## Design Considerations

### Memory Efficiency
- âœ… **Zero-copy views** implemented (view, reshape, transpose)
- âŒ **Broadcasting** needs view-based implementation (avoid copies)
- âŒ **Slicing** should return views, not copies

### Performance
- âœ… **Matmul** highly optimized (OpenBLAS + transpose detection)
- âš ï¸ **Element-wise ops** could benefit from vectorization (AVX/SIMD)
- âŒ **Reductions** need parallel implementation
- âŒ **Convolutions** need im2col or Winograd optimization

### API Design
- âœ… **Method chaining** works well (`A.transpose().contiguous()`)
- âš ï¸ **In-place ops** missing (need `_` suffix: `A.add_(B)`)
- âŒ **Operator overloading** could be extended (`A[i:j]`)

---

## Comparison with Major Frameworks

| Feature | cpptensor | PyTorch | NumPy | TensorFlow | Status |
|---------|-----------|---------|-------|------------|--------|
| **Basic arithmetic** | âœ… | âœ… | âœ… | âœ… | Complete |
| **Matmul** | âœ… | âœ… | âœ… | âœ… | Optimized |
| **Views/reshaping** | âœ… | âœ… | âœ… | âœ… | Complete |
| **Reductions** | âŒ | âœ… | âœ… | âœ… | Missing |
| **Broadcasting** | âŒ | âœ… | âœ… | âœ… | Missing |
| **Slicing** | âŒ | âœ… | âœ… | âœ… | Missing |
| **Convolutions** | âŒ | âœ… | âŒ | âœ… | Missing |
| **Autograd** | 10% | âœ… | âŒ | âœ… | Partial |
| **GPU support** | âŒ | âœ… | âŒ | âœ… | Planned |

---

## Contributing Guidelines

### Adding New Operations

1. **Create header file:** `include/cpptensor/ops/<category>/<op>.hpp`
2. **Implement:** `src/ops/<category>/<op>.cpp`
3. **Add dispatcher:** Update backend dispatcher if needed
4. **Write tests:** Add to test suite (once test infrastructure is fixed)
5. **Document:** Add to this catalog
6. **Benchmark:** Compare with PyTorch/NumPy

### Operation Template

```cpp
// include/cpptensor/ops/category/operation.hpp
#pragma once
#include "cpptensor/tensor/tensor.hpp"

namespace cpptensor {
    /**
     * @brief Brief description
     * 
     * @param A Input tensor
     * @param param Operation parameter
     * @return Tensor Result
     */
    Tensor operation(const Tensor& A, int param);
}

// src/ops/category/operation.cpp
#include "cpptensor/ops/category/operation.hpp"

namespace cpptensor {
    Tensor operation(const Tensor& A, int param) {
        // Implementation
        // 1. Validate inputs
        // 2. Allocate output
        // 3. Call backend/kernel
        // 4. Return result
    }
}
```

---

## FAQ

**Q: Why are reductions missing?**  
A: Reductions require careful dimension handling and are the next priority.

**Q: When will autograd be complete?**  
A: Backward pass implementation is planned for Phase 3-4. Infrastructure exists.

**Q: Is CUDA support planned?**  
A: Yes, but after core CPU operations are complete. Backend system is designed for it.

**Q: Why no broadcasting?**  
A: Complex feature affecting all operations. Being implemented in Phase 1.

**Q: Can I use this for production?**  
A: Currently suitable for research/prototyping. Production use after Phase 4.

---

## Resources

- **PyTorch Docs:** https://pytorch.org/docs/stable/torch.html
- **NumPy Reference:** https://numpy.org/doc/stable/reference/
- **Project README:** [README.md](README.md)
- **Performance Analysis:** [PERFORMANCE_ANALYSIS.md](PERFORMANCE_ANALYSIS.md)
- **View Architecture:** [docs/VIEW_ARCHITECTURE.md](docs/VIEW_ARCHITECTURE.md)

---

*Last updated: November 5, 2025*  
*Maintainer: cpptensor team*  
*License: MIT*
