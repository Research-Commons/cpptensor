# View-Based Tensor Architecture

This document explains the zero-copy view architecture implementation in cpptensor, including the design rationale, memory management, and implementation details.

## Table of Contents

- [Overview](#overview)
- [Core Architecture Changes](#core-architecture-changes)
- [The `base_impl_` Design](#the-base_impl_-design)
- [Tensor Manipulation Functions](#tensor-manipulation-functions)
- [Memory Layout Examples](#memory-layout-examples)
- [Key Design Principles](#key-design-principles)
- [Alternative Designs](#alternative-designs)

---

## Overview

cpptensor implements a **PyTorch-style zero-copy view architecture** for tensor manipulation operations. This enables efficient tensor reshaping, transposition, and dimension reordering without copying data.

**Key Benefits:**
- ‚úÖ **Zero-copy operations**: View, reshape, permute, transpose without data duplication
- ‚úÖ **Automatic memory management**: Safe shared ownership via `shared_ptr`
- ‚úÖ **PyTorch compatibility**: Familiar API and semantics
- ‚úÖ **Contiguity tracking**: Automatic detection and handling of memory layout

---

## Core Architecture Changes

### TensorImpl: Added View Support

**Files:** `include/cpptensor/tensor/tensorimpl.hpp`, `src/tensor/tensorimpl.cpp`

#### New Member Variable

```cpp
std::shared_ptr<TensorImpl> base_impl_;
```

**Purpose:**
- When a tensor is a "view", this points to the original tensor that owns the data
- Keeps the base tensor alive as long as any view exists (shared ownership)
- Enables zero-copy operations by sharing data between tensors

#### New View Constructor

```cpp
TensorImpl(std::shared_ptr<TensorImpl> base,
           const std::vector<size_t>& new_shape,
           const std::vector<size_t>& new_stride = {});
```

**Behavior:**
- Creates views that **share data** instead of copying
- Takes a base tensor, new shape, and optionally new strides
- If `new_stride` is empty, computes default row-major strides
- Sets `base_impl_` to keep base alive via shared ownership

#### Data Delegation

```cpp
const std::vector<float>& TensorImpl::data() const { 
    if (base_impl_) {
        return base_impl_->data();  // Views delegate to base
    }
    return data_;  // Original tensors use their own data
}
```

**Key Insight:**
- **Views don't store their own data** - they delegate to the base tensor
- This enables **zero-copy operations**: views and base share the same memory
- Recursive delegation: views of views work correctly (chains to root)

---

## The `base_impl_` Design

### Why `base_impl_` is Needed

The `base_impl_` pointer solves a **critical memory management problem** with shared data in views.

#### ‚ùå Problem Without `base_impl_`

```cpp
void problematic_example() {
    Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});  // Create original tensor
    Tensor B = A.view({3, 2});              // Create view
    
    // At this point:
    // - A.impl_ points to TensorImpl with data [1,2,3,4,5,6]
    // - B.impl_ points to NEW TensorImpl that needs access to A's data
    
    // A goes out of scope here...
}  // üí• A is destroyed, its data_ vector is freed!

// B still exists but its data is now INVALID - use after free!
```

**The Problem:** When A is destroyed, its `data_` vector is freed. If B just had a pointer to A's data, that pointer would be dangling.

#### ‚úÖ Solution WITH `base_impl_`

```cpp
void safe_example() {
    Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
    Tensor B = A.view({3, 2});
    
    // At this point:
    // - A.impl_ = shared_ptr<TensorImpl> (owns data)
    // - B.impl_ = shared_ptr<TensorImpl> (view)
    // - B.impl_->base_impl_ = A.impl_ (shared_ptr keeping A alive!)
    
    // A goes out of scope...
}  // ‚úÖ A.impl_ not destroyed because B.impl_->base_impl_ still holds a reference!

// B is still valid! The original data is kept alive by shared_ptr
```

---

### Memory Ownership Diagrams

#### Scenario 1: Normal Tensor (No Views)

```cpp
Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
```

```
Stack:                          Heap:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tensor A‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄshared_ptr‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ TensorImpl           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ  ‚îú‚îÄ data_: [1,2,3,..] ‚îÇ ‚óÄ‚îÄ‚îÄ Owns the data
                               ‚îÇ  ‚îú‚îÄ base_impl_: null  ‚îÇ ‚óÄ‚îÄ‚îÄ No base (original)
                               ‚îÇ  ‚îú‚îÄ shape_: [2, 3]    ‚îÇ
                               ‚îÇ  ‚îî‚îÄ stride_: [3, 1]   ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               Reference count = 1
```

**What happens when A goes out of scope?**
- `A.impl_` is destroyed (shared_ptr)
- Reference count drops to 0
- TensorImpl is deleted
- `data_` vector is freed ‚úÖ

---

#### Scenario 2: Tensor With View

```cpp
Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
Tensor B = A.view({3, 2});
```

```
Stack:                          Heap:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tensor A‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄshared_ptr‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ TensorImpl (Base)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ           ‚îÇ  ‚îú‚îÄ data_: [1,2,3,..] ‚îÇ ‚óÄ‚îÄ‚îÄ Owns the data
                   ‚îÇ           ‚îÇ  ‚îú‚îÄ base_impl_: null  ‚îÇ
                   ‚îÇ           ‚îÇ  ‚îú‚îÄ shape_: [2, 3]    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ           ‚îÇ  ‚îî‚îÄ stride_: [3, 1]   ‚îÇ
‚îÇ Tensor B‚îÇ‚îÄ‚îÄ‚îê     ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ           Reference count = 2 ‚óÄ‚îÄ‚îÄ‚îê
             ‚îÇ     ‚îÇ                                   ‚îÇ
       shared_ptr  ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
             ‚îÇ     ‚îÇ           ‚îÇ TensorImpl (View)    ‚îÇ
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ‚îú‚îÄ data_: EMPTY     ‚îÇ
                   ‚îÇ           ‚îÇ  ‚îú‚îÄ base_impl_‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚óÄ‚îÄ‚îÄ Points to base!
                   ‚îÇ           ‚îÇ  ‚îú‚îÄ shape_: [3, 2]    ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  ‚îî‚îÄ stride_: [2, 1]   ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Points:**
1. **Base TensorImpl has reference count = 2** (A and B's view both point to it)
2. **View's `base_impl_` keeps base alive** via `shared_ptr`
3. **View's `data_` is EMPTY** - it delegates to base

---

#### Scenario 3: Original Tensor Dies, View Lives

```cpp
Tensor B;
{
    Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
    B = A.view({3, 2});
    // A goes out of scope here...
}
// B is still alive and valid!
```

```
After A is destroyed:

Stack:                          Heap:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tensor B‚îÇ‚îÄ‚îÄ‚îê                 ‚îÇ TensorImpl (Base)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                 ‚îÇ  ‚îú‚îÄ data_: [1,2,3,..] ‚îÇ ‚óÄ‚îÄ‚îÄ Still alive!
             ‚îÇ                 ‚îÇ  ‚îú‚îÄ base_impl_: null  ‚îÇ
       shared_ptr               ‚îÇ  ‚îú‚îÄ shape_: [2, 3]    ‚îÇ
             ‚îÇ                 ‚îÇ  ‚îî‚îÄ stride_: [3, 1]   ‚îÇ
             ‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                 Reference count = 1 ‚óÄ‚îÄ‚îÄ‚îê
             ‚îÇ                                         ‚îÇ
             ‚îÇ                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ TensorImpl (View)    ‚îÇ
                               ‚îÇ  ‚îú‚îÄ data_: EMPTY     ‚îÇ
                               ‚îÇ  ‚îú‚îÄ base_impl_‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ  ‚îú‚îÄ shape_: [3, 2]    ‚îÇ
                               ‚îÇ  ‚îî‚îÄ stride_: [2, 1]   ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What happened?**
1. A's `impl_` shared_ptr was destroyed
2. But reference count only dropped from 2 ‚Üí 1 (B's view still holds reference via `base_impl_`)
3. **Data stays alive!** ‚úÖ
4. B can still access data through `base_impl_->data()`

---

### Data Access Flow

Let's trace what happens when you access data from a view:

```cpp
Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
Tensor B = A.view({3, 2});
float val = B.data()[0];  // What happens here?
```

**Call Chain:**
```cpp
1. B.data()
   ‚Üì
2. B.impl_->data()  // TensorImpl::data()
   ‚Üì
3. if (base_impl_) { return base_impl_->data(); }  // View case!
   ‚Üì
4. base_impl_->data()  // Call data() on base
   ‚Üì
5. if (base_impl_) { ... } else { return data_; }  // Base case!
   ‚Üì
6. return data_;  // Returns actual data vector
```

**Recursive View Example:**
```cpp
Tensor A({2, 3, 4}, ...);
Tensor B = A.view({6, 4});      // B.base_impl_ = A.impl_
Tensor C = B.view({24});        // C.base_impl_ = B.impl_
Tensor D = C.view({3, 8});      // D.base_impl_ = C.impl_
```

**Chain of Views:**
```
D.base_impl_ ‚îÄ‚îÄ‚ñ∂ C.impl_
                 C.base_impl_ ‚îÄ‚îÄ‚ñ∂ B.impl_
                                  B.base_impl_ ‚îÄ‚îÄ‚ñ∂ A.impl_
                                                   A.base_impl_ = null
                                                   A.data_ = [actual data]
```

When you call `D.data()`:
```cpp
D.data()
  ‚Üí D.base_impl_->data()     // C.impl_
    ‚Üí C.base_impl_->data()   // B.impl_
      ‚Üí B.base_impl_->data() // A.impl_
        ‚Üí A.data_            // Finally reaches actual data!
```

---

### What Happens to Original Tensor If No Views?

#### Case 1: No Views Created

```cpp
{
    Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
    // Do some operations...
    float sum = A.data()[0] + A.data()[1];
}  // A goes out of scope
```

**Destruction Sequence:**
1. `A` destructor called
2. `A.impl_` (shared_ptr) destructor called
3. Reference count = 0 (only A had reference)
4. `TensorImpl` destructor called
5. `data_` vector destroyed
6. Memory freed ‚úÖ

**`base_impl_` is null**, so it doesn't affect anything.

---

#### Case 2: Views Were Created But Also Destroyed

```cpp
{
    Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
    {
        Tensor B = A.view({3, 2});
        // Use B...
    }  // B destroyed here (reference count: 2 ‚Üí 1)
    // A still alive
}  // A destroyed here (reference count: 1 ‚Üí 0)
```

**Timeline:**
- Initially: A.impl_ reference count = 1
- After creating B: A.impl_ reference count = 2 (A and B's `base_impl_`)
- B destroyed: A.impl_ reference count = 2 ‚Üí 1
- A destroyed: A.impl_ reference count = 1 ‚Üí 0 ‚Üí memory freed ‚úÖ

---

## Tensor Manipulation Functions

### 1. `view(new_shape)` - Zero-Copy Reshape

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::view(const std::vector<size_t>& new_shape) const {
    // 1. Validate total elements match
    // 2. Check tensor is contiguous (REQUIRED for view)
    // 3. Create view TensorImpl sharing data with impl_
    auto view_impl = std::make_shared<TensorImpl>(impl_, new_shape);
    return Tensor(view_impl);
}
```

**Characteristics:**
- **Zero-copy**: No data duplication
- **Requirement**: Tensor must be contiguous (throws error if not)
- **Shares data**: Modifying view modifies original

**Example:**
```cpp
Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
Tensor B = A.view({3, 2});  // Shape: {3, 2}, shares data with A
B.data()[0] = 99;            // A.data()[0] is now also 99!
```

---

### 2. `reshape(new_shape)` - Smart Reshape

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::reshape(const std::vector<size_t>& new_shape) const {
    if (is_contiguous()) {
        return view(new_shape);  // Zero-copy path
    } else {
        return contiguous().view(new_shape);  // Copy then view
    }
}
```

**Smart Behavior:**
- **Contiguous tensor** ‚Üí Uses `view()` (zero-copy)
- **Non-contiguous tensor** ‚Üí Makes contiguous copy first, then views
- **User-friendly**: Works in all cases automatically

**Example:**
```cpp
Tensor A({2, 3}, ...);
Tensor B = A.reshape({6});     // Zero-copy if A is contiguous

Tensor C = A.transpose();       // C is non-contiguous
Tensor D = C.reshape({6});      // Makes a copy first
```

---

### 3. `flatten(start_dim, end_dim)` - Dimension Flattening

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::flatten(int start_dim, int end_dim) const {
    // 1. Normalize negative indices
    // 2. Validate dimension range
    // 3. Build new shape:
    //    - Keep dims before start_dim
    //    - Flatten dims from start_dim to end_dim into one
    //    - Keep dims after end_dim
    return reshape(new_shape);
}
```

**Examples:**
```cpp
Tensor A({2, 3, 4}, ...);

Tensor B = A.flatten();         // Shape: {24} (flatten all)
Tensor C = A.flatten(1, 2);     // Shape: {2, 12} (flatten dims 1-2)
Tensor D = A.flatten(0, 1);     // Shape: {6, 4} (flatten dims 0-1)
```

---

### 4. `squeeze(dim)` - Remove Size-1 Dimensions

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::squeeze(int dim) const {
    if (dim == -1) {
        // Remove ALL dimensions of size 1
    } else {
        // Remove specific dimension (must be size 1)
    }
    return reshape(new_shape);
}
```

**Examples:**
```cpp
Tensor A({2, 1, 3, 1, 4}, ...);

Tensor B = A.squeeze();         // Shape: {2, 3, 4} (remove all size-1)
Tensor C = A.squeeze(1);        // Shape: {2, 3, 1, 4} (remove dim 1)
Tensor D = A.squeeze(3);        // Shape: {2, 1, 3, 4} (remove dim 3)
```

---

### 5. `unsqueeze(dim)` - Add Size-1 Dimension

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::unsqueeze(int dim) const {
    // Normalize dimension (allow negative indexing)
    // Insert dimension of size 1 at specified position
    return reshape(new_shape);
}
```

**Examples:**
```cpp
Tensor A({2, 3}, ...);

Tensor B = A.unsqueeze(0);      // Shape: {1, 2, 3}
Tensor C = A.unsqueeze(1);      // Shape: {2, 1, 3}
Tensor D = A.unsqueeze(-1);     // Shape: {2, 3, 1}
```

---

### 6. `permute(dims)` - Arbitrary Dimension Reordering

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::permute(const std::vector<int>& dims) const {
    // 1. Validate: dims must be valid permutation of [0..ndim-1]
    // 2. Reorder shape and stride according to permutation
    std::vector<size_t> new_shape(ndims);
    std::vector<size_t> new_stride(ndims);
    for (int i = 0; i < ndims; ++i) {
        int d = dims[i];
        new_shape[i] = old_shape[d];
        new_stride[i] = old_stride[d];  // Key: reorder strides!
    }
    // 3. Create view with modified shape AND stride
    auto view_impl = std::make_shared<TensorImpl>(impl_, new_shape, new_stride);
    return Tensor(view_impl);
}
```

**Zero-Copy Magic:**
- **Doesn't copy data** - only changes strides
- Result is **non-contiguous** (strides no longer row-major)
- Can be made contiguous with `.contiguous()`

**Examples:**
```cpp
Tensor A({2, 3, 4}, ...);

Tensor B = A.permute({2, 0, 1});  // Shape: {4, 2, 3}, non-contiguous
Tensor C = A.permute({1, 0, 2});  // Swap first two dims
Tensor D = A.permute({2, 1, 0});  // Reverse dimension order
```

---

### 7. `transpose(dim0, dim1)` - Swap Two Dimensions

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::transpose(int dim0, int dim1) const {
    // Default: swap last two dimensions (classic 2D transpose)
    // Create permutation that swaps dim0 and dim1
    std::vector<int> perm(ndims);
    for (int i = 0; i < ndims; ++i) perm[i] = i;
    std::swap(perm[dim0], perm[dim1]);
    return permute(perm);  // Reuse permute implementation
}
```

**Convenience wrapper around permute:**

**Examples:**
```cpp
Tensor A({3, 4}, ...);
Tensor B = A.transpose();         // Shape: {4, 3} (swap last two dims)

Tensor C({2, 3, 4}, ...);
Tensor D = C.transpose(0, 2);     // Shape: {4, 3, 2} (swap dims 0 and 2)
Tensor E = C.transpose(0, 1);     // Shape: {3, 2, 4} (swap dims 0 and 1)
```

---

### 8. `is_contiguous()` - Check Memory Layout

**Location:** `src/tensor/tensor.cpp`

```cpp
bool Tensor::is_contiguous() const {
    // Check if strides match expected row-major layout
    size_t expected_stride = 1;
    for (int i = sh.size() - 1; i >= 0; --i) {
        if (st[i] != expected_stride) return false;
        expected_stride *= sh[i];
    }
    return true;
}
```

**Row-Major Check:**
- For shape `{2, 3, 4}`, expected strides are `{12, 4, 1}`
- After `permute()` or `transpose()`, strides change ‚Üí non-contiguous

**Examples:**
```cpp
Tensor A({2, 3}, ...);
A.is_contiguous();                // true (newly created)

Tensor B = A.transpose();
B.is_contiguous();                // false (strides changed)

Tensor C = B.contiguous();
C.is_contiguous();                // true (made contiguous)
```

---

### 9. `contiguous()` - Fix Memory Layout

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::contiguous() const {
    if (is_contiguous()) {
        return *this;  // Already contiguous, no copy needed
    }
    
    // Copy data in row-major order
    // Iterate through logical indices
    // Read from non-contiguous layout using strides
    // Write to contiguous layout
    return Tensor(sh, new_data, device_type());
}
```

**Smart Copying:**
- If already contiguous ‚Üí returns self (no copy)
- If non-contiguous ‚Üí creates new tensor with contiguous (row-major) layout

**Examples:**
```cpp
Tensor A({2, 3}, ...);
Tensor B = A.transpose();         // Non-contiguous

Tensor C = B.contiguous();        // New contiguous tensor
C.is_contiguous();                // true
```

---

### 10. `clone()` - Deep Copy

**Location:** `src/tensor/tensor.cpp`

```cpp
Tensor Tensor::clone() const {
    // Deep copy - create new data buffer
    return Tensor(shape(), impl_->data(), device_type());
}
```

**Simple Deep Copy:**
- Always creates independent copy
- Views and clones are completely independent

**Examples:**
```cpp
Tensor A({2, 3}, ...);
Tensor B = A.view({3, 2});        // View shares data
Tensor C = A.clone();             // Clone has independent data

A.data()[0] = 99;
// B.data()[0] == 99  (shares data)
// C.data()[0] != 99  (independent)
```

---

## Memory Layout Examples

### Original Tensor

```cpp
Tensor A({2, 3}, {1, 2, 3, 4, 5, 6});
```

```
Memory: [1, 2, 3, 4, 5, 6]
Shape:  [2, 3]
Strides: [3, 1]  (contiguous, row-major)

Logical view:
[[1, 2, 3],
 [4, 5, 6]]

Access: A[i,j] = memory[i*3 + j*1]
```

---

### View (Zero-Copy)

```cpp
Tensor B = A.view({3, 2});
```

```
Memory: [1, 2, 3, 4, 5, 6]  (SAME as A)
Shape:  [3, 2]
Strides: [2, 1]

Logical view:
[[1, 2],
 [3, 4],
 [5, 6]]

Access: B[i,j] = memory[i*2 + j*1]
Example: B[1,0] = memory[1*2 + 0*1] = memory[2] = 3
```

**Key Point:** Same memory, different interpretation via strides!

---

### Permute (Zero-Copy, Non-Contiguous)

```cpp
Tensor C = A.transpose();  // Shape: {3, 2}
```

```
Memory: [1, 2, 3, 4, 5, 6]  (SAME as A)
Shape:  [3, 2]
Strides: [1, 3]  (non-contiguous!)

Logical view:
[[1, 4],
 [2, 5],
 [3, 6]]

Access: C[i,j] = memory[i*1 + j*3]
Examples:
  C[0,0] = memory[0*1 + 0*3] = memory[0] = 1
  C[0,1] = memory[0*1 + 1*3] = memory[3] = 4
  C[1,0] = memory[1*1 + 0*3] = memory[1] = 2
  C[1,1] = memory[1*1 + 1*3] = memory[4] = 5
```

**Key Point:** Non-contiguous strides allow column-major access to row-major data!

---

### Contiguous Copy

```cpp
Tensor D = C.contiguous();  // Shape: {3, 2}
```

```
Memory: [1, 4, 2, 5, 3, 6]  (NEW buffer, reordered)
Shape:  [3, 2]
Strides: [2, 1]  (contiguous again)

Logical view:
[[1, 4],
 [2, 5],
 [3, 6]]

Access: D[i,j] = memory[i*2 + j*1]
Example: D[1,0] = memory[1*2 + 0*1] = memory[2] = 2
```

**Key Point:** Data physically reordered to match logical view!

---

## Key Design Principles

### 1. Zero-Copy When Possible

Operations like `view()`, `permute()`, and `transpose()` don't copy data:
```cpp
Tensor A({1000, 1000}, ...);  // 1M elements
Tensor B = A.transpose();      // Instant! No copy
Tensor C = B.permute({1, 0});  // Instant! No copy
```

Achieved via shared ownership (`shared_ptr<TensorImpl>`) and stride manipulation.

---

### 2. Contiguity Tracking

Operations that require contiguous memory:
- Automatically detect contiguity with `is_contiguous()`
- Call `contiguous()` when needed
- Throw errors if unsafe (e.g., `view()` on non-contiguous tensor)

```cpp
Tensor A({2, 3}, ...);
Tensor B = A.transpose();      // Non-contiguous
// B.view({6});                // Would throw error
Tensor C = B.contiguous();     // Make contiguous
Tensor D = C.view({6});        // Now safe
```

---

### 3. Smart Operations

Operations automatically choose the most efficient path:

**reshape():**
```cpp
if (is_contiguous()) {
    return view(new_shape);    // Zero-copy
} else {
    return contiguous().view(new_shape);  // Copy first
}
```

**contiguous():**
```cpp
if (is_contiguous()) {
    return *this;              // No-op
} else {
    return copy_and_reorder(); // Make contiguous
}
```

---

### 4. PyTorch Compatibility

API matches PyTorch for easy migration:

| Operation | cpptensor | PyTorch |
|-----------|-----------|---------|
| Zero-copy reshape | `view()` | `view()` |
| Smart reshape | `reshape()` | `reshape()` |
| Transpose | `transpose()` | `transpose()` |
| Permute | `permute()` | `permute()` |
| Check layout | `is_contiguous()` | `is_contiguous()` |
| Fix layout | `contiguous()` | `contiguous()` |
| Deep copy | `clone()` | `clone()` |

---

## Alternative Designs

### Why Not Raw Pointers?

```cpp
TensorImpl* base_impl_;  // ‚ùå Bad!
```

**Problem:** No ownership management ‚Üí use-after-free when base is destroyed

**Example:**
```cpp
Tensor view;
{
    Tensor base({2, 3}, ...);
    view = base.view({3, 2});
}  // base destroyed, view.base_impl_ is dangling pointer!
view.data();  // üí• Use-after-free
```

---

### Why Not Copy Data?

```cpp
data_ = base->data_;  // ‚ùå Bad!
```

**Problem:** Not zero-copy ‚Üí defeats the purpose of views

**Performance:**
```cpp
Tensor A({1000, 1000}, ...);  // 1M elements = 4MB

// With views (our implementation):
Tensor B = A.transpose();      // 0 bytes copied, instant

// Without views (copying):
Tensor B = A.transpose();      // 4MB copied, slow
```

---

### Why Not Always Point to Root?

```cpp
// Skip intermediate views, point directly to root
base_impl_ = find_root(base);  // ü§î Could work, but...
```

**Problems:**
- More complex logic
- Breaks encapsulation (view needs to know about entire chain)
- Our current solution is simpler and equally correct

**Current Solution:**
```cpp
// Simple: each view points to its immediate parent
base_impl_ = base;  // ‚úÖ Simple and works
```

The chain naturally follows through: `view‚Üíparent‚Üígrandparent‚Üí...‚Üíroot`

---

## Summary

### Key Insights

1. **`base_impl_` enables safe view semantics**
    - Keeps base data alive via `shared_ptr`
    - No manual reference counting needed
    - Automatic cleanup when all views destroyed

2. **Zero-copy operations via stride manipulation**
    - Permute/transpose just reorder strides
    - No data duplication
    - Orders of magnitude faster for large tensors

3. **Smart automatic behavior**
    - Operations choose optimal path (view vs. copy)
    - Contiguity tracking prevents errors
    - PyTorch-compatible API

4. **Minimal overhead**
    - Original tensors: no extra cost (`base_impl_` is null)
    - Views: one extra `shared_ptr` (8 bytes on 64-bit systems)

---

### The Magic One-Liner

```cpp
std::shared_ptr<TensorImpl> base_impl_;  // This enables:
// ‚úÖ Safe view semantics
// ‚úÖ Zero-copy operations
// ‚úÖ Automatic memory management
// ‚úÖ No manual reference counting
```

An elegant solution leveraging C++'s `shared_ptr` to achieve PyTorch-like view behavior with automatic memory management!

---

## References

- **Implementation files:**
    - `include/cpptensor/tensor/tensor.hpp`
    - `include/cpptensor/tensor/tensorimpl.hpp`
    - `src/tensor/tensor.cpp`
    - `src/tensor/tensorimpl.cpp`

- **Tests:**
    - `test/test_tensor_manipulation.cpp`

- **Examples:**
    - `examples/tensor_example.cpp`

- **Documentation:**
    - `TENSOR_MANIPULATION_IMPLEMENTATION.md`
